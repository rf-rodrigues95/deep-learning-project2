{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2d970b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Environment.snake_game import SnakeGame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import imageio # For video generation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.005\n",
    "EPSILON_DECAY = 0.995\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "TARGET_UPDATE_FREQ = 100\n",
    "EVAL_INTERVAL = 500\n",
    "\n",
    "GAME_WIDTH = 30\n",
    "GAME_HEIGHT = 30\n",
    "FOOD_AMOUNT = 1\n",
    "GRASS_GROWTH = 0.001\n",
    "MAX_GRASS = 0.05\n",
    "BORDER_SIZE = 1 # Add a visual border of 1 pixel\n",
    "MAX_GAME_STEPS = 500 # Set a limit for the game length\n",
    "VIDEO_FILENAME = \"snake_ai_player.gif\"\n",
    "FPS = 5 # Faster for smoother video\n",
    "\n",
    "TEMPERATURE_END = 0.1\n",
    "TEMPERATURE_START = 2.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82479ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Heuristic Player\n",
    "\"\"\"\n",
    "class SnakeHeuristic:\n",
    "    def __init__(self, game: SnakeGame):\n",
    "        self.game = game\n",
    "        # Map: game_direction_idx -> (row_change, col_change)\n",
    "        self.delta_coords = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (0, 1),   # Right\n",
    "            2: (1, 0),   # Down\n",
    "            3: (0, -1)   # Left\n",
    "        }\n",
    "\n",
    "    def _get_closest_apple_pos(self):\n",
    "        if not self.game.apples:\n",
    "            return None\n",
    "\n",
    "        head_r, head_c = self.game.snake[0]\n",
    "        closest_apple = None\n",
    "        min_dist = float('inf')\n",
    "\n",
    "        for apple_r, apple_c in self.game.apples:\n",
    "            dist = abs(apple_r - head_r) + abs(apple_c - head_c)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                closest_apple = (apple_r, apple_c)\n",
    "        return closest_apple\n",
    "\n",
    "    def _is_safe_move(self, next_head_r, next_head_c):\n",
    "        if not (0 <= next_head_r < self.game.height and \\\n",
    "                0 <= next_head_c < self.game.width):\n",
    "            return False\n",
    "\n",
    "        if (next_head_r, next_head_c) in self.game.snake[1:]:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def choose_action(self):\n",
    "        target_apple_pos = self._get_closest_apple_pos()\n",
    "\n",
    "        if target_apple_pos is None:\n",
    "            return 0\n",
    "\n",
    "        head_r, head_c = self.game.snake[0]\n",
    "        current_game_direction = self.game.direction\n",
    "\n",
    "        candidate_actions = []\n",
    "\n",
    "        for action_value in [-1, 0, 1]:\n",
    "            next_game_direction = (current_game_direction + action_value) % 4\n",
    "            if next_game_direction < 0: next_game_direction += 4\n",
    "\n",
    "            dr, dc = self.delta_coords[next_game_direction]\n",
    "\n",
    "            next_head_r, next_head_c = head_r + dr, head_c + dc\n",
    "\n",
    "            if self._is_safe_move(next_head_r, next_head_c):\n",
    "                dist_to_apple = abs(target_apple_pos[0] - next_head_r) + \\\n",
    "                                abs(target_apple_pos[1] - next_head_c)\n",
    "                candidate_actions.append({\n",
    "                    'action': action_value,\n",
    "                    'distance': dist_to_apple\n",
    "                })\n",
    "\n",
    "        if not candidate_actions:\n",
    "            return 0\n",
    "\n",
    "        def sort_key(candidate):\n",
    "            action = candidate['action']\n",
    "            preference = 0\n",
    "            if action == 1: preference = 1\n",
    "            elif action == -1: preference = 2\n",
    "            return (candidate['distance'], preference)\n",
    "\n",
    "        candidate_actions.sort(key=sort_key)\n",
    "\n",
    "        return candidate_actions[0]['action']\n",
    "\n",
    "    def play_game_and_record(self, max_steps: int, video_filename: str = \"snake_heuristic_game.gif\", fps: int = 5):\n",
    "        frames = []\n",
    "        history = {\n",
    "            'board': [],\n",
    "            'reward': [],\n",
    "            'done': -1,\n",
    "            'info': []\n",
    "        }\n",
    "        board_state, reward, done, info = self.game.reset()\n",
    "\n",
    "        frames.append((board_state * 255).astype(np.uint8))\n",
    "\n",
    "        print(f\"Starting game with heuristic. Max steps: {max_steps}. Recording to {video_filename}\")\n",
    "\n",
    "        for step_num in range(max_steps):\n",
    "            history['board'].append(board_state)\n",
    "            history['reward'].append(reward)\n",
    "            history['info'].append(info)\n",
    "            if done:\n",
    "                history['done'] = step_num\n",
    "                print(f\"Game ended prematurely at step {step_num} before taking action. Score: {info['score']}\")\n",
    "                break\n",
    "\n",
    "            action_to_take = self.choose_action()\n",
    "            board_state, reward, done, info = self.game.step(action_to_take)\n",
    "\n",
    "            frames.append((board_state * 255).astype(np.uint8))\n",
    "\n",
    "            if (step_num + 1) % 100 == 0:\n",
    "                 print(f\"Step {step_num+1}/{max_steps}, Score: {info['score']}, Done: {done}\")\n",
    "\n",
    "\n",
    "            \"\"\"if done:\n",
    "                print(f\"Game over at step {step_num+1}. Final Score: {info['score']}\")\n",
    "                break\"\"\"\n",
    "        else:\n",
    "            print(f\"Game finished after {max_steps} steps (max_steps reached). Final Score: {self.game.score}\")\n",
    "\n",
    "        if frames:\n",
    "            print(f\"Saving video with {len(frames)} frames at {fps} FPS...\")\n",
    "            try:\n",
    "                imageio.mimsave(video_filename, frames, fps=fps)\n",
    "                print(f\"Video saved successfully as {video_filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving video: {e}\")\n",
    "                print(\"Please ensure you have an imageio backend installed (e.g., 'pip install imageio[ffmpeg]' for MP4 or 'pip install imageio pillow' for GIF).\")\n",
    "        else:\n",
    "            print(\"No frames recorded.\")\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15a811fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "    DQN, Replay Buffer, and Helper Functions\n",
    "\"\"\"\n",
    "class SnakeDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(SnakeDQN, self).__init__()\n",
    "        C, H, W = input_shape\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(C, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.zeros(1, *input_shape)\n",
    "        conv_out_size = self.conv(dummy_input).view(1, -1).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def plot_training_metrics(scores, losses, q_values, fig_name=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(6, 12))  # Adjusted for vertical layout\n",
    "\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(scores)\n",
    "    plt.title(\"Scores\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Average Loss per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(q_values)\n",
    "    plt.title(\"Average Q-value per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Q-value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if fig_name:\n",
    "        plt.savefig(fig_name)\n",
    "    plt.show()\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        return (\n",
    "            torch.tensor(np.array(states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity=10000, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, td_error=None):\n",
    "        # Priority = abs(td_error) or max priority if td_error is None\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        priority = abs(td_error) + 1e-5 if td_error is not None else max_priority\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "\n",
    "        self.priorities[self.pos] = priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            probs = self.priorities\n",
    "        else:\n",
    "            probs = self.priorities[:len(self.buffer)]\n",
    "\n",
    "        probs = probs ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(np.array(states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            torch.tensor(weights, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            indices\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        for idx, error in zip(indices, td_errors):\n",
    "            self.priorities[idx] = abs(error) + 1e-5\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac008b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 | Score: 0.10 | Loss: 0.3498 | Q-value: 0.0232 | Eps: 0.005\n",
      "Episode 20 | Score: 0.30 | Loss: 0.1406 | Q-value: -0.0112 | Eps: 0.005\n",
      "Episode 30 | Score: 0.65 | Loss: 0.0624 | Q-value: -0.0972 | Eps: 0.005\n",
      "Episode 40 | Score: 1.20 | Loss: 0.0403 | Q-value: -0.0392 | Eps: 0.005\n",
      "Episode 50 | Score: 2.10 | Loss: 0.0237 | Q-value: -0.0987 | Eps: 0.005\n",
      "Episode 50, Avg Score: 0.51, Epsilon: 0.005\n",
      "Episode 60 | Score: 0.35 | Loss: 0.1279 | Q-value: 0.0306 | Eps: 0.005\n",
      "Episode 70 | Score: 0.65 | Loss: 0.0484 | Q-value: -0.1256 | Eps: 0.005\n",
      "Episode 80 | Score: 0.35 | Loss: 0.0715 | Q-value: -0.1780 | Eps: 0.005\n",
      "Episode 90 | Score: 0.65 | Loss: 0.0095 | Q-value: -0.3851 | Eps: 0.005\n",
      "Episode 100 | Score: 0.90 | Loss: 0.0062 | Q-value: -0.1864 | Eps: 0.005\n",
      "Episode 100, Avg Score: 0.78, Epsilon: 0.005\n",
      "Episode 110 | Score: 1.00 | Loss: 0.0119 | Q-value: -0.1606 | Eps: 0.005\n",
      "Episode 120 | Score: 0.85 | Loss: 0.0092 | Q-value: -0.2675 | Eps: 0.005\n",
      "Episode 130 | Score: 1.15 | Loss: 0.0232 | Q-value: 0.1348 | Eps: 0.005\n",
      "Episode 140 | Score: 0.10 | Loss: 0.0046 | Q-value: -0.9342 | Eps: 0.005\n",
      "Episode 150 | Score: 0.10 | Loss: 0.0118 | Q-value: -0.8178 | Eps: 0.005\n",
      "Episode 150, Avg Score: 0.67, Epsilon: 0.005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    105\u001b[39m model = SnakeDQN(state_shape, n_actions=\u001b[32m3\u001b[39m).to(device)\n\u001b[32m    107\u001b[39m start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m trained_model, scores, losses, q_values = \u001b[43mtrain_dqn_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musing_heuristic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheuristic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheuristic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m end = time.time()\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Plot\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mtrain_dqn_simple\u001b[39m\u001b[34m(env, model, using_heuristic, heuristic)\u001b[39m\n\u001b[32m     56\u001b[39m done_tensor = torch.tensor([done], dtype=torch.float32).to(device)\n\u001b[32m     57\u001b[39m action_tensor = torch.tensor([[-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m].index(action)], dtype=torch.int64).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m pred_q = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m loss = compute_q_loss(pred_q, action_tensor, reward_tensor, next_state_tensor, done_tensor, model)\n\u001b[32m     62\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mSnakeDQN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     28\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv(x)\n\u001b[32m     29\u001b[39m x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def evaluate_efficiency(scores, q_values, total_time):\n",
    "    avg_score = np.mean(scores)\n",
    "    avg_q = np.mean(q_values)\n",
    "    print(f\"--- Evaluation Summary ---\")\n",
    "    print(f\"Average Score: {avg_score:.2f}\")\n",
    "    print(f\"Average Q-value: {avg_q:.4f}\")\n",
    "    print(f\"Training Time: {total_time:.2f} seconds\")\n",
    "    print(f\"Score / Time: {avg_score / total_time:.4f}\")\n",
    "    print(f\"Q-value / Time: {avg_q / total_time:.4f}\")\n",
    "\n",
    "def compute_q_loss(pred_q, action, reward, next_state, done, model, gamma=0.99):\n",
    "    with torch.no_grad():\n",
    "        next_q = model(next_state)\n",
    "        max_next_q = next_q.max(1)[0]\n",
    "        target_q = reward + gamma * max_next_q * (1 - done)\n",
    "\n",
    "    pred_q_action = pred_q.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    loss = F.mse_loss(pred_q_action, target_q)\n",
    "    return loss\n",
    "\n",
    "\"\"\"\n",
    "    Task 1 - train simple\n",
    "\"\"\"\n",
    "def train_dqn_simple(env, model, using_heuristic=False, heuristic=None):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    epsilon = 0.005\n",
    "\n",
    "    scores = []\n",
    "    losses = []\n",
    "    q_values = []\n",
    "\n",
    "    max_train_episodes = 1000\n",
    "    max_total_steps = max_train_episodes*MAX_GAME_STEPS\n",
    "\n",
    "    for episode in range(1, max_train_episodes + 1):\n",
    "        state, _, done, _ = env.reset()\n",
    "        state = np.transpose(state, (2, 0, 1))\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        total_loss = 0\n",
    "        total_q = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        while not done and steps < MAX_GAME_STEPS:\n",
    "            # Epsilon-greedy action\n",
    "            if random.random() < epsilon:\n",
    "                if using_heuristic:\n",
    "                    action = heuristic.choose_action()\n",
    "                    #action = heuristic_move(env)\n",
    "                else:\n",
    "                    action = random.choice([-1, 0, 1])\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = model(state)\n",
    "                    action_idx = q_vals.argmax().item()\n",
    "                    action = [-1, 0, 1][action_idx]\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            next_state = np.transpose(next_state, (2, 0, 1))\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            reward_tensor = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "            done_tensor = torch.tensor([done], dtype=torch.float32).to(device)\n",
    "            action_tensor = torch.tensor([[-1, 0, 1].index(action)], dtype=torch.int64).to(device)\n",
    "\n",
    "            pred_q = model(state)\n",
    "            loss = compute_q_loss(pred_q, action_tensor, reward_tensor, next_state_tensor, done_tensor, model)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action_index = torch.tensor([[[-1, 0, 1].index(action)]], dtype=torch.int64).to(device)\n",
    "                action_index = action_index.view(1, 1)\n",
    "                q_val_taken = pred_q.gather(1, action_index).mean().item()\n",
    "                total_q += q_val_taken\n",
    "\n",
    "            total_loss += loss.detach()\n",
    "            state = next_state_tensor\n",
    "            steps += 1\n",
    "\n",
    "\n",
    "        scores.append(info['score'])\n",
    "        losses.append(float(total_loss.cpu()) / steps if steps > 0 else 0)\n",
    "        q_values.append(total_q / steps if steps > 0 else 0)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode} | Score: {info['score']:.2f} | Loss: {losses[-1]:.4f} | Q-value: {q_values[-1]:.4f} | Eps: {epsilon:.3f}\")\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            avg_score = sum(scores[-50:]) / 50\n",
    "            print(f\"Episode {episode}, Avg Score: {avg_score:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    return model, scores, losses, q_values\n",
    "\n",
    "\n",
    "game_instance = SnakeGame(\n",
    "    width=GAME_WIDTH,\n",
    "    height=GAME_HEIGHT,\n",
    "    food_amount=FOOD_AMOUNT,\n",
    "    border=BORDER_SIZE,\n",
    "    grass_growth=GRASS_GROWTH,\n",
    "    max_grass=MAX_GRASS)\n",
    "\n",
    "state = game_instance.reset()[0]\n",
    "state = np.transpose(state, (2, 0, 1))\n",
    "state_shape = state.shape\n",
    "\n",
    "heuristic = SnakeHeuristic(game_instance)\n",
    "## TASK 1\n",
    "model = SnakeDQN(state_shape, n_actions=3).to(device)\n",
    "\n",
    "start = time.time()\n",
    "trained_model, scores, losses, q_values = train_dqn_simple(game_instance, model, using_heuristic=False, heuristic=heuristic)\n",
    "end = time.time()\n",
    "\n",
    "# Plot\n",
    "plot_training_metrics(scores, losses, q_values)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_efficiency(scores, q_values, total_time=end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deac9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltzmann_action(q_values, temperature):\n",
    "    probabilities = F.softmax(q_values / temperature, dim=1)\n",
    "    return torch.multinomial(probabilities, 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a1d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv_dqn(env, model, decay_pattern, using_heuristic=False, heuristic=None, input_shape=None, n_actions=None, bufferType=\"BasicReplay\", exploration_style = \"epsilon\"):\n",
    "    target_model = type(model)(input_shape, n_actions).to(device)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    target_model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    epsilon_max = 0.001\n",
    "    epsilon_min = 0.001\n",
    "    epsilon_decay = 0.990\n",
    "    buffer = ReplayBuffer(capacity=10000) if bufferType == \"BasicReplay\" else PrioritizedReplayBuffer(capacity=10000)\n",
    "    #buffer = PrioritizedReplayBuffer(capacity=10000)\n",
    "    scores = []\n",
    "    avg_scores = []\n",
    "    losses = []\n",
    "    q_values = []\n",
    "    epsilons = []\n",
    "\n",
    "    max_train_episodes = 1000\n",
    "    update_target_every = 100\n",
    "    batch_size = 32\n",
    "    gamma = 0.99\n",
    "    beta_frames = 100000\n",
    "    global_step = 0\n",
    "    epsilon = epsilon_max\n",
    "    decay_per_step = 0.05\n",
    "    epsilon_decay_rate = 0.99\n",
    "    MAX_STEPS = max_train_episodes * MAX_GAME_STEPS\n",
    "    temperature_decay_rate = -np.log(TEMPERATURE_END / TEMPERATURE_START) / MAX_STEPS\n",
    "\n",
    "    temperature=TEMPERATURE_START\n",
    "    is_prioritized = isinstance(buffer, PrioritizedReplayBuffer)\n",
    "    print(f\"we are using {bufferType} buffer\")\n",
    "\n",
    "    # WARMUP\n",
    "    if using_heuristic and heuristic is not None:\n",
    "        print(\"Starting warmup with heuristic for 50 episodes...\")\n",
    "        for _ in range(50):\n",
    "            state, _, done, _ = env.reset()\n",
    "            state = np.transpose(state, (2, 0, 1))\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            done = False\n",
    "            steps = 0\n",
    "            while not done and steps < MAX_GAME_STEPS:\n",
    "                action = heuristic.choose_action()\n",
    "\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state_proc = np.transpose(next_state, (2, 0, 1))\n",
    "\n",
    "                buffer.add(state.cpu().squeeze(0).numpy(), [-1, 0, 1].index(action), reward, next_state_proc, float(done))\n",
    "\n",
    "                state = torch.tensor(next_state_proc, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                steps += 1\n",
    "\n",
    "        print(f\"Warmup finished. Replay buffer size: {len(buffer)}\")\n",
    "\n",
    "    for episode in range(1, max_train_episodes + 1):\n",
    "        state, _, done, _ = env.reset()\n",
    "        state = np.transpose(state, (2, 0, 1))\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        total_loss = 0\n",
    "        total_q = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        while not done and steps < MAX_GAME_STEPS:\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice([-1, 0, 1])\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = model(state)\n",
    "                    if exploration_style == \"boltzmann\":\n",
    "                        action_idx = boltzmann_action(q_vals, temperature)\n",
    "                    else:\n",
    "                        action_idx = q_vals.argmax().item()\n",
    "                    action = [-1, 0, 1][action_idx]\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state_proc = np.transpose(next_state, (2, 0, 1))\n",
    "            next_state_tensor = torch.tensor(next_state_proc, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            if is_prioritized:\n",
    "                with torch.no_grad():\n",
    "                    q_val = model(state).gather(1, torch.tensor([[[-1, 0, 1].index(action)]]).to(device))\n",
    "                    next_q = target_model(next_state_tensor).max(1)[0].unsqueeze(1)\n",
    "                    td_error = reward + gamma * (1 - float(done)) * next_q - q_val\n",
    "                    td_error = td_error.item()\n",
    "\n",
    "                buffer.add(\n",
    "                    state.cpu().squeeze(0).numpy(),\n",
    "                    [-1, 0, 1].index(action),\n",
    "                    reward,\n",
    "                    next_state_proc,\n",
    "                    float(done),\n",
    "                    td_error=td_error\n",
    "                )\n",
    "            else:\n",
    "                buffer.add(\n",
    "                    state.cpu().squeeze(0).numpy(),\n",
    "                    [-1, 0, 1].index(action),\n",
    "                    reward,\n",
    "                    next_state_proc,\n",
    "                    float(done)\n",
    "                )\n",
    "\n",
    "            state = next_state_tensor\n",
    "            steps += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # Learn from replay buffer\n",
    "            if len(buffer) >= batch_size:\n",
    "                if is_prioritized:\n",
    "                    b_states, b_actions, b_rewards, b_next_states, b_dones, weights, indices = buffer.sample(batch_size)\n",
    "                else:\n",
    "                    b_states, b_actions, b_rewards, b_next_states, b_dones = buffer.sample(batch_size)\n",
    "\n",
    "                q_values_pred = model(b_states).gather(1, b_actions)\n",
    "                with torch.no_grad():\n",
    "                    next_qs = target_model(b_next_states)\n",
    "                    max_next_qs = next_qs.max(1, keepdim=True)[0]\n",
    "                    q_target = b_rewards + gamma * (1 - b_dones) * max_next_qs\n",
    "\n",
    "                td_errors = q_target - q_values_pred\n",
    "                if is_prioritized:\n",
    "                    loss = (td_errors.pow(2) * weights).mean()\n",
    "                else:\n",
    "                    loss = nn.MSELoss()(q_values_pred, q_target)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if isinstance(buffer, PrioritizedReplayBuffer):\n",
    "                    buffer.update_priorities(indices, td_errors.detach().cpu().numpy().squeeze())\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_q += q_values_pred.mean().item()\n",
    "\n",
    "            # Target network update\n",
    "            if global_step % update_target_every == 0:\n",
    "                target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        # Decay epsilon after each episode\n",
    "        #if epsilon > epsilon_min:\n",
    "         #   epsilon *= epsilon_decay\n",
    "\n",
    "        epsilons.append(epsilon)\n",
    "        if exploration_style == \"boltzmann\":\n",
    "            temperature = max(TEMPERATURE_END, TEMPERATURE_START * np.exp(-temperature_decay_rate * global_step))\n",
    "        elif exploration_style == \"epsilon\":\n",
    "            if decay_pattern == \"linear\":\n",
    "                #epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "                epsilon = max(epsilon_min, epsilon_max - (3 * epsilon_max * episode / max_train_episodes)) # 3x so its faster decaying\n",
    "            elif decay_pattern == \"exponential\":\n",
    "                epsilon = max(epsilon_min, epsilon * epsilon_decay_rate)\n",
    "            elif decay_pattern == \"step\":\n",
    "                if episode % 10 == 0:\n",
    "                    epsilon = max(epsilon_min, epsilon - decay_per_step)\n",
    "        else:\n",
    "            print(\"Wrong decay pattern\")\n",
    "\n",
    "        scores.append(info['score'])\n",
    "        losses.append(total_loss / steps if steps > 0 else 0)\n",
    "        q_values.append(total_q / steps if steps > 0 else 0)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode} | Score: {info['score']} | Loss: {losses[-1]:.4f} | Q: {q_values[-1]:.4f}\")\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            avg_score = sum(scores[-50:]) / 50\n",
    "            avg_scores.append(avg_score)\n",
    "            print(f\"Episode {episode}, Avg Score: {avg_score:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    return model, avg_scores, losses, q_values, epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac148b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are using BasicReplay buffer\n",
      "Starting warmup with heuristic for 50 episodes...\n",
      "Warmup finished. Replay buffer size: 10000\n",
      "Episode 10 | Score: 1.2500000000000004 | Loss: 0.0779 | Q: 0.3740\n",
      "Episode 20 | Score: 1.958000000000001 | Loss: 0.0744 | Q: 0.5161\n",
      "Episode 30 | Score: 1.1500000000000004 | Loss: 0.0816 | Q: 0.7727\n",
      "Episode 40 | Score: 0.25 | Loss: 0.0598 | Q: 1.0166\n",
      "Episode 50 | Score: 0.35 | Loss: 0.0886 | Q: 1.1874\n",
      "Episode 50, Avg Score: 0.82, Epsilon: 0.001\n",
      "Episode 60 | Score: 0.1 | Loss: 0.0425 | Q: 1.4397\n",
      "Episode 70 | Score: 1.6500000000000008 | Loss: 0.1248 | Q: 1.6930\n",
      "Episode 80 | Score: 0.35 | Loss: 0.2675 | Q: 1.8728\n",
      "Episode 90 | Score: 0.7000000000000001 | Loss: 0.1386 | Q: 2.0088\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m heuristic = SnakeHeuristic(game_instance)\n\u001b[32m     15\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m trained_model, avg_scores, losses, q_values, epsilons = \u001b[43mtrain_adv_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgame_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecay_pattern\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_heuristic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheuristic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheuristic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexploration_style\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mboltzmann\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m end_time = time.time()\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining time: \u001b[39m\u001b[33m\"\u001b[39m, end_time-start_time)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mtrain_adv_dqn\u001b[39m\u001b[34m(env, model, decay_pattern, using_heuristic, heuristic, input_shape, n_actions, bufferType, exploration_style)\u001b[39m\n\u001b[32m    119\u001b[39m q_values_pred = model(b_states).gather(\u001b[32m1\u001b[39m, b_actions)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     next_qs = \u001b[43mtarget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_next_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     max_next_qs = next_qs.max(\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m    123\u001b[39m     q_target = b_rewards + gamma * (\u001b[32m1\u001b[39m - b_dones) * max_next_qs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mSnakeDQN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:213\u001b[39m, in \u001b[36mMaxPool2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/_jit_internal.py:622\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_workspace/.venv/lib/python3.12/site-packages/torch/nn/functional.py:830\u001b[39m, in \u001b[36m_max_pool2d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    829\u001b[39m     stride = torch.jit.annotate(\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "game_instance = SnakeGame(\n",
    "    width=GAME_WIDTH,\n",
    "    height=GAME_HEIGHT,\n",
    "    food_amount=FOOD_AMOUNT,\n",
    "    border=BORDER_SIZE,\n",
    "    grass_growth=GRASS_GROWTH,\n",
    "    max_grass=MAX_GRASS)\n",
    "state = game_instance.reset()[0]\n",
    "state = np.transpose(state, (2, 0, 1))\n",
    "state_shape = state.shape\n",
    "model = SnakeDQN(state_shape, n_actions=3).to(device)\n",
    "\n",
    "heuristic = SnakeHeuristic(game_instance)\n",
    "\n",
    "start_time = time.time()\n",
    "trained_model, avg_scores, losses, q_values, epsilons = train_adv_dqn(\n",
    "    game_instance,\n",
    "    model,\n",
    "    decay_pattern=\"linear\",\n",
    "    using_heuristic=True,\n",
    "    heuristic=heuristic,\n",
    "    input_shape=state_shape,\n",
    "    n_actions=3,\n",
    "    exploration_style=\"boltzmann\"\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Training time: \", end_time-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
